{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlowAlpha/SPIRES_kenya_sensing/blob/main/benin_export_tiles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esIMGVxhDI0f"
      },
      "source": [
        "#@title Copyright 2020 Google LLC. { display-mode: \"form\" }\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1xZ1CPi3Nw"
      },
      "source": [
        "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
        "<a target=\"_blank\"  href=\"http://colab.research.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SHAc5qbiR8l"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is an Earth Engine <> TensorFlow demonstration notebook. The model is a [fully convolutional neural network (FCNN)](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf), specifically [U-net](https://arxiv.org/abs/1505.04597). This notebook shows:\n",
        "\n",
        "1.   Exporting training/testing patches from Earth Engine, suitable for training an FCNN model.\n",
        "2.   Preprocessing.\n",
        "3.   Training and validating an FCNN model.\n",
        "4.   Making predictions with the trained model and importing them to Earth Engine.\n",
        "\n",
        "This notebook implements a UNet model for prediction in Benin from Landsat7 data. Major code chunks borrowed from.\n",
        "\n",
        "\n",
        "1.   https://csaybar.github.io/blog/2019/06/21/eetf2/\n",
        "2.   https://developers.google.com/earth-engine/guides/tf_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ4kW1pEhwP"
      },
      "source": [
        "# Setup software libraries\n",
        "\n",
        "Authenticate and import as necessary.\n",
        "\n",
        "IMPORTANT: When training make sure that you are connected to a GPU runtime. When uploading data you do not have to be. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#After running this cell, restart your kernel and run it again to properly import geemap\n",
        "#import subprocess\n",
        "\n",
        "#try:\n",
        "#    import geemap\n",
        "#except ImportError:\n",
        "#    print('Installing geemap ...')\n",
        "#    subprocess.check_call([\"python\", '-m', 'pip', 'install', 'geemap'])\n",
        "#    !pip install earthengine-api"
      ],
      "metadata": {
        "id": "as3xyWjhClJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neIa46CpciXq"
      },
      "source": [
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jat01FEoUMqg"
      },
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RnZzcYhcpsQ"
      },
      "source": [
        "# Tensorflow setup.\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "# Folium setup.\n",
        "import folium\n",
        "print(folium.__version__)\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKs6HuxOzjMl"
      },
      "source": [
        "## Specify your Cloud Storage Bucket\n",
        "You must have write access to a bucket to run this demo!  To run it read-only, use the demo bucket below, but note that writes to this bucket will not work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obDDH1eDzsch"
      },
      "source": [
        "# INSERT YOUR BUCKET HERE:\n",
        "BUCKET = 'jp_bucket_1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfKLl9XcnGJ"
      },
      "source": [
        "## Set other global variables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify the bands we want to use for prediction. Here we select R,G,B values as well as bands to calculate the NDVI index with. Select kernel size based on the tile size you would like your model to train on"
      ],
      "metadata": {
        "id": "ncMJ5NMteJBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opticalBands = ['B3','B2','B1'] #RGB\n",
        "thermalBands = ['B4','B3'] #NIR\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "\n",
        "BANDS = ['R', 'G', 'B', 'NDVI']\n",
        "RESPONSE = 'target'\n",
        "FEATURES = BANDS + [RESPONSE] \n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 128\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "\n",
        "\n",
        "#Download Benin Data and create Raster of villages\n",
        "\n",
        "#Import Voronoi Raster here:\n",
        "voronoi = ee.FeatureCollection('projects/sanford-project-04a9/assets/voronoi_villages')\n",
        "\n",
        "treated_voronoi = voronoi.filter(ee.Filter.eq('treated', 1))\n",
        "\n",
        "#Get Benin Feature collection\n",
        "benin = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\").filter(ee.Filter.eq('country_na','Benin')).set('ORIG_FID',0)\n",
        "treated_voronoi_rast = treated_voronoi.filter(ee.Filter.notNull(['VID'])).reduceToImage(properties=['VID'],reducer= ee.Reducer.first())\n",
        "\n",
        "# Create a village mask based on the treated village raster\n",
        "villagemask = treated_voronoi_rast.mask() #select(['ORIG_FID']).gt(.1)\n"
      ],
      "metadata": {
        "id": "2CKizdZZeH-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgoDc7Hilfc4"
      },
      "source": [
        "# Imagery\n",
        "\n",
        "Gather and setup the imagery to use for inputs (predictors).  This is a three-year, cloud-free, Landsat 7 composite. Landsat 7 contains imagery pre 2009 lottery.  Display it in the notebook for a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get sample points we will train on\n",
        "#sample_points = ee.FeatureCollection('projects/sanford-project-04a9/assets/128_pixel_tiles').sort('random')\n",
        "#Get centroids of patches we want to pull from\n",
        "sample_points = ee.FeatureCollection(\"projects/sanford-project-04a9/assets/128_pixel_centroids\")\n",
        "#64_pixel_tiles_only_villages is only the tiles that are 100% within a village so they donâ€™t require any masking and there are only 4494 of them"
      ],
      "metadata": {
        "id": "lh-o0YQf3uDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display Grids that we will use for training\n",
        "sample_points\n",
        "mapid = sample_points.getMapId({'bands': ['R', 'G', 'B'], 'min': 0, 'max': 3000})\n",
        "map = folium.Map(location=[9.8, 2.4], zoom_start = 7)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "metadata": {
        "id": "jI3xTm60Fmc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Import Benin Imagery to make predictions\n",
        "def cloudMaskL457(image): \n",
        "  qa = image.select('pixel_qa')\n",
        "  # If the cloud bit (5) is set and the cloud confidence (7) is high\n",
        "  # or the cloud shadow bit is set (3), then it's a bad pixel.\n",
        "  cloud = qa.bitwiseAnd(1 << 5).And(qa.bitwiseAnd(1 << 7)).Or(qa.bitwiseAnd(1 << 3))\n",
        "  # Remove edge pixels that don't occur in all bands\n",
        "  mask2 = image.mask().reduce(ee.Reducer.min())\n",
        "  return image.updateMask(cloud.Not()).updateMask(mask2)\n",
        "\n",
        "#Select Geometry from all of Benin or just treatment and control circles\n",
        "\n",
        "#This geometry selects from all of Benin\n",
        "image = ee.ImageCollection(\"LANDSAT/LE07/C01/T1_SR\").filterDate('2007-01-01', '2008-12-31')\n",
        "#image = (image.map(cloudMaskL457).median().clip(benin.geometry().buffer(10000)).reproject(crs = ee.Projection('EPSG:32631'), scale=30))\n",
        "#image = (image.map(cloudMaskL457).median().setDefaultProjection(image.first().projection()).clip(benin.geometry().buffer(10000)))\n",
        "image = (image.map(cloudMaskL457).median().clip(benin.geometry().buffer(10000)))\n",
        "\n",
        "#This geometry selects only from villages\n",
        "#image = ee.ImageCollection(\"LANDSAT/LE07/C01/T1_SR\").filterDate('2005-01-01', '2006-12-31').map(cloudMaskL457).median().clip(voronoi.geometry())\n",
        "\n",
        "#This geometry selects only from list of evenly spaced points created by Luke\n",
        "#image = ee.ImageCollection(\"LANDSAT/LE07/C01/T1_SR\").filterDate('2005-01-01', '2006-12-31').map(cloudMaskL457).filterBounds(sample_points).median()\n",
        "\n",
        "\n",
        "image_ndvi = image.normalizedDifference(thermalBands).rename(['NDVI'])\n",
        "image_rgb = image.select(opticalBands).rename(['R','G','B']) \n",
        "image = image_rgb.addBands(image_ndvi)\n",
        "image = image.unmask(0)\n"
      ],
      "metadata": {
        "id": "vfevvBgEJb7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Running this code will display the imagery\n",
        "mapid = image.getMapId({'bands': ['R', 'G', 'B'], 'min': 0, 'max': 3000})\n",
        "map = folium.Map(location=[9.8, 2.4], zoom_start = 7)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "metadata": {
        "id": "GwBl7BAIFF1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is using a version of cloud masking from Landsat8. We should stick to the version adapted to Landsat7 previously, although in some cases the landsat8 cloud masking looks better."
      ],
      "metadata": {
        "id": "oc0_dLHqnfjy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IlgXu-vcUEY"
      },
      "source": [
        "# Use Landsat 7 surface reflectance data.\n",
        "# l7sr = ee.ImageCollection(\"LANDSAT/LE07/C01/T1_SR\")\n",
        "\n",
        "# # Cloud masking function. From Landsat 8 but may want to find a filtering function specifically for Landsat 7\n",
        "# def maskL8sr(image):\n",
        "#   cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "#   cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "#   qa = image.select('pixel_qa')\n",
        "#   mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "#     qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "#   mask2 = image.mask().reduce('min')\n",
        "#   mask3 = image.select(opticalBands).gt(0).And(\n",
        "#           image.select(opticalBands).lt(10000)).reduce('min')\n",
        "#   mask = mask1.And(mask2).And(mask3)\n",
        "#   return image.select(opticalBands).divide(10000).addBands(\n",
        "#           image.select(thermalBands).divide(10).clamp(273.15, 373.15)\n",
        "#             .subtract(273.15).divide(100)).updateMask(mask)\n",
        "\n",
        "# # The image input data is a cloud-masked median composite. Get data from pre 2009.\n",
        "# image_2 = l7sr.filterDate('2005-01-01', '2006-12-31').map(maskL8sr).median().clip(benin.geometry().buffer(3000))\n",
        "\n",
        "# image_2_ndvi = image_2.normalizedDifference(thermalBands).rename(['NDVI'])\n",
        "# image_2_rgb = image_2.select(opticalBands).rename(['R','G','B']) \n",
        "# image_2 = image_2_rgb.addBands(image_ndvi)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #Use folium to visualize the imagery.\n",
        "# mapid = image_2.getMapId({'bands': ['R', 'G', 'B'], 'min': 0, 'max': .3})\n",
        "# map = folium.Map(location=[9.8, 2.4], zoom_start = 7)\n",
        "# folium.TileLayer(\n",
        "#     tiles=mapid['tile_fetcher'].url_format,\n",
        "#     attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "#     overlay=True,\n",
        "#     name='median composite',\n",
        "#   ).add_to(map)\n",
        "# map.add_child(folium.LayerControl())\n",
        "# map\n",
        "#Optional to add NDVI Visualization\n",
        "# mapid = image.getMapId({'bands': ['NDVI'], 'min': 0, 'max': 0.5})\n",
        "# folium.TileLayer(\n",
        "#     tiles=mapid['tile_fetcher'].url_format,\n",
        "#     attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "#     overlay=True,\n",
        "#     name='thermal',\n",
        "#   ).add_to(map)\n",
        "# map.add_child(folium.LayerControl())\n",
        "# map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHznnctkJsZJ"
      },
      "source": [
        "Prepare the response (what we want to predict).  This is either treatment if we are looking to estimate propensity score or landcover in 2020. For predicting treatment we add on a target band marked as 1 within villages. For landcover in 2020 we add in bands specifying the land type. Only run one code chunk and comment out the other."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this Code if you'd like to export the training imagery to google earth enginge for visualization.\n",
        "# import time \n",
        "# # Export the image to an Earth Engine asset.\n",
        "# task = ee.batch.Export.image.toAsset(**{\n",
        "#   'image': image,\n",
        "#   'description': 'imageToAssetExample',\n",
        "#   'assetId': 'users/jacksonpullman/BeninImagery',\n",
        "#   'scale': 100,\n",
        "#   'region': benin.geometry().getInfo()['coordinates']\n",
        "# })\n",
        "# task.start()\n",
        "# while task.active():\n",
        "#   print('Polling for task (id: {}).'.format(task.id))\n",
        "#   time.sleep(5)"
      ],
      "metadata": {
        "id": "bN6hDeDlQozI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0wHDyxVirec"
      },
      "source": [
        "#Code to add in bands for propensity score prediction \n",
        "l7Masked = image.updateMask(villagemask)\n",
        "l7Unmasked = l7Masked.unmask(-9999)\n",
        "outside_circle = 'b(\"R\") > -9000'\n",
        "target = l7Unmasked.expression(outside_circle).rename(\"target\")\n",
        "\n",
        "\n",
        "#Code to add in bands for landcover prediction\n",
        "#target = ee.ImageCollection(\"ESA/WorldCover/v100\").max().rename('target').divide(100).float().clip(benin.geometry().buffer(3000))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize imagery\n",
        "mapid = target.getMapId({'min': 0, 'max': 1})\n",
        "map = folium.Map(location=[9.8, 2.4], zoom_start = 7)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='target variable',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "metadata": {
        "id": "ecog0S8kCNof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTS7_ZzPDhhg"
      },
      "source": [
        "Stack the 2D images (Landsat composite and response) to create a single image from which samples can be taken.  Convert the image into an array image in which each pixel stores 64x64 patches of pixels for each band.  This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGHYsdAOipa4"
      },
      "source": [
        "#Combine response and image band values for training\n",
        "featureStack = ee.Image.cat([\n",
        "  image.select(BANDS),\n",
        "  target.select(RESPONSE),\n",
        "]).float()\n",
        "\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "arrays = featureStack.neighborhoodToArray(kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4djSxBRG2el"
      },
      "source": [
        "Use some pre-made geometries to sample the stack in strategic locations.  Specifically, these are hand-made polygons in which to take the 256x256 samples.  Display the sampling polygons on a map, red for training polygons, blue for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV890gPHeZqz"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "The mapped data look reasonable so take a sample from each polygon and merge the results into a single export.  The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point.  It's worth noting that to build the training and testing data for the FCNN, you export a single TFRecord file that contains patches of pixel values in each record.  You do NOT need to export each training/testing patch to a different image.  Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the `computed value too large` error.  Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export. THIS CODE CAN TAKE A WHILE TO RUN (10 minutes) MAKE SURE YOU CHECK THE FILES ARE UPLOADED TO GOOGLE CLOUD BEFOE CONTINUING."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify names locations for outputs in Cloud Storage. \n",
        "FOLDER = 'unet_2-23_128_tiles_benin_villages_r_g_b_ndvi_treatment'\n",
        "TRAINING_BASE = 'training_patches'\n",
        "EVAL_BASE = 'eval_patches'"
      ],
      "metadata": {
        "id": "1es1N_oCnEuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def saveCNN_batch(image, point,kernel_size,scale,FilePrefix, selectors,folder, bucket= BUCKET):\n",
        "  \"\"\"\n",
        "    Export a dataset for semantic segmentation by batches\n",
        "  \n",
        "  Params:\n",
        "  ------\n",
        "    - image : ee.Image to get pixels from; must be scalar-valued.\n",
        "    - point : Points to sample over.\n",
        "    - kernel_size : The kernel specifying the shape of the neighborhood. Only fixed, square and rectangle kernels are supported.\n",
        "      Weights are ignored; only the shape of the kernel is used.\n",
        "    - scale : A nominal scale in meters of the projection to work in.\n",
        "    - FilePrefix : Cloud Storage object name prefix for the export.\n",
        "    - selector : Specified the properties to save.\n",
        "    - bucket : The name of a Cloud Storage bucket for the export.  \n",
        "  \"\"\"\n",
        "  print('Found Cloud Storage bucket.' if tf.io.gfile.exists('gs://' + bucket) \n",
        "    else 'Output Cloud Storage bucket does not exist.')\n",
        "  \n",
        "  # Download the points (Server -> Client)\n",
        "  nbands = len(selectors)\n",
        "  points = point.geometry().getInfo()['coordinates']    \n",
        "  nfeatures = kernel_size*kernel_size*nbands*len(points) #estimate the totals # of features\n",
        "     \n",
        "  image_neighborhood = arrays\n",
        "  filenames = []\n",
        "  \n",
        "  #Threshold considering the max number of features permitted to export.\n",
        "  if nfeatures > 3e6:\n",
        "    nparts = int(np.ceil(nfeatures/3e6))\n",
        "    print('Dataset too long, splitting it into '+ str(nparts),'equal parts.')\n",
        "    \n",
        "    nppoints = np.array(points)\n",
        "    np.random.shuffle(nppoints)\n",
        "    \n",
        "    count_batch = 1  # Batch counter \n",
        "    \n",
        "    for batch_arr in np.array_split(nppoints,nparts):\n",
        "      \n",
        "      fcp = ee.FeatureCollection([\n",
        "          ee.Feature(ee.Geometry.Point(p),{'class':'NA'}) \n",
        "          for p in batch_arr.tolist() \n",
        "      ])\n",
        "      \n",
        "      # Agriculture dataset (fcp-points) collocation to each L5 grid cell value.\n",
        "      train_db = image_neighborhood.sampleRegions(collection=fcp, scale=scale)\n",
        "\n",
        "      if(count_batch == 1):\n",
        "        full_train_db = train_db\n",
        "      else:\n",
        "        full_train_db.merge(train_db)\n",
        "\n",
        "\n",
        "\n",
        "      filename = '%s/%s-%04d_' % (folder,FilePrefix,count_batch)\n",
        "      \n",
        "      # Create the tasks for passing of GEE to Google storage\n",
        "    print('sending the task #%04d'%count_batch)\n",
        "    Task = ee.batch.Export.table.toCloudStorage(\n",
        "        collection=full_train_db,        \n",
        "        selectors=selectors,          \n",
        "        description='Export batch '+str(count_batch),\n",
        "        fileNamePrefix=filename,\n",
        "        bucket=bucket,  \n",
        "        fileFormat='TFRecord')\n",
        "      \n",
        "    Task.start()\n",
        "    filenames.append(filename)\n",
        "    count_batch+=1\n",
        "      \n",
        "    while Task.active():\n",
        "      print('Polling for task (id: {}).'.format(Task.id))\n",
        "      time.sleep(3)\n",
        "        \n",
        "    return filenames\n",
        "  \n",
        "#  else:    \n",
        "#    train_db = image_neighborhood.sampleRegions(collection=points, scale=scale)         \n",
        "#    Task = ee.batch.Export.table.toCloudStorage(\n",
        "#      collection=train_db,\n",
        "#      selectors=selectors,\n",
        "#      description='Training Export',\n",
        "#      fileNamePrefix=FilePrefix,\n",
        "#      bucket=bucket,  \n",
        "#      fileFormat='TFRecord')\n",
        "#    Task.start()\n",
        "#    \n",
        "#    while Task.active():\n",
        "#      print('Polling for task (id: {}).'.format(Task.id))\n",
        "#      time.sleep(3)\n",
        "    \n",
        "#    return FilePrefix"
      ],
      "metadata": {
        "id": "DfhaTCmB9R6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Want to update this to upload feature collection in bulk, instead of 60,000 separate items."
      ],
      "metadata": {
        "id": "TOnF8ijXdRJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide training and testing points\n",
        "train_points = sample_points.filter(ee.Filter.lt('random', 0.66))\n",
        "test_points = sample_points.filter(ee.Filter.gt('random', 0.66))\n",
        "print(train_points.size().getInfo())\n",
        "print(test_points.size().getInfo())"
      ],
      "metadata": {
        "id": "HSwakgi7j0ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selectors = ['R','G','B','NDVI','target']\n",
        "train_filenames = saveCNN_batch(featureStack,train_points,128,30,'trainUNET', selectors,folder =FOLDER)\n",
        "test_filenames = saveCNN_batch(featureStack,test_points,128,30,'testUNET', selectors,folder =FOLDER)\n",
        "#Occasionannly computed value is too large, so we will chunk our upload into groups of 300 tiles"
      ],
      "metadata": {
        "id": "X1mv5MZ1m_fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrvBE4607G"
      },
      "source": [
        "# Training data\n",
        "\n",
        "Load the data exported from Earth Engine into a `tf.data.Dataset`.  The following are helper functions for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWZ0UXCVMyJP"
      },
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "  Returns:\n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "  \"\"\"\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "  Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "  Returns:\n",
        "    A tuple of (inputs, outputs).\n",
        "  \"\"\"\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  # Convert from CHW to HWC\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "\n",
        "\n",
        "def get_dataset(pattern):\n",
        "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "  Get all the files matching the pattern, parse and convert to tuple.\n",
        "  Args:\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "  Returns:\n",
        "    A tf.data.Dataset\n",
        "  \"\"\"\n",
        "  glob = tf.io.gfile.glob(pattern)\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg1fa18336D2"
      },
      "source": [
        "Use the helpers to read in the training dataset.  Print the first record to check."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "xynnJTMZIGy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm0qRF0fAYcC"
      },
      "source": [
        "def get_training_dataset():\n",
        "\t\"\"\"Get the preprocessed training dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of training data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + \"train\" + '*'\n",
        "\t#glob = 'gs://' + BUCKET + '/' + \"unet_10-28\" + '/' + TRAINING_BASE + '*'\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\t#dataset = dataset.batch(BATCH_SIZE)\n",
        "\t#dataset = dataset.batch(1).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "training = get_training_dataset()\n",
        "\n",
        "print(iter(training.take(1)).next())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-cQO5RL6vob"
      },
      "source": [
        "# Evaluation data\n",
        "\n",
        "Now do the same thing to get an evaluation dataset.  Note that unlike the training dataset, the evaluation dataset has a batch size of 1, is not repeated and is not shuffled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieKTCGiJ6xzo"
      },
      "source": [
        "def get_eval_dataset():\n",
        "\t\"\"\"Get the preprocessed evaluation dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of evaluation data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + \"test\" + '*'\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.batch(1).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "evaluation = get_eval_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(iter(evaluation.take(1)).next())"
      ],
      "metadata": {
        "id": "GXiIM8fhOJ8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIE7Yl87lgU"
      },
      "source": [
        "# Model\n",
        "\n",
        "Here we use the Keras implementation of the U-Net model.  The U-Net model takes 256x256 pixel patches as input and outputs per-pixel class probability, label or a continuous output. Depending on what we are trying to predict (treatment or land cover) we can choose to treat this as a classification or regression problem. For propensity score estimation here we choose to use a custom Binary Cross Entropy Dice Loss Function and for regression we can use typical Mean Square Error. Can Choose to change this."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Custom Losses\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "\n",
        "def dice_coeff(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    # Flatten\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "    return score\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "  loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "1whAcWgSjo8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify model training parameters.\n",
        "#OPTIMIZER = 'SGD'\n",
        "OPTIMIZER = 'adam'\n",
        "#LOSS = 'MeanSquaredError'\n",
        "LOSS = 'BinaryCrossentropy'\n",
        "#METRICS = ['RootMeanSquaredError']\n",
        "METRICS = ['AUC']"
      ],
      "metadata": {
        "id": "DLW5J_Ncmpul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsnnnz56yS3l"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[None, None, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\t#Change final layer activation based on what you are predicting\n",
        "\t#outputs = layers.Conv2D(1, (1, 1), activation='linear')(decoder0)\n",
        "\toutputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizers.get(OPTIMIZER), \n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu_E7OTDBCoS"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "You train a Keras model by calling `.fit()` on it.  Here we're going to train for 10 epochs, which is suitable for demonstration purposes.  For production use, you probably want to optimize this parameter, for example through [hyperparamter tuning](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import capabilities to save model to drive\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# Callbacks time\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "es = EarlyStopping(monitor='val_loss', patience=5)\n",
        "mcp = ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Run this cell to mount your Google Drive.\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZS_kukhAqOTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = train_points.size().getInfo()\n",
        "EVAL_SIZE = test_points.size().getInfo()\n",
        "print(EVAL_SIZE)\n",
        "print(TRAIN_SIZE)"
      ],
      "metadata": {
        "id": "qL1SR9RVIqGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzzaWxOhSxBy"
      },
      "source": [
        "m = get_model()\n",
        "\n",
        "history = m.fit(\n",
        "    x=training, \n",
        "    epochs=5, \n",
        "    steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n",
        "    validation_data=evaluation,\n",
        "    validation_steps=EVAL_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2XrwZHp66j4"
      },
      "source": [
        "Note that the notebook VM is sometimes not heavy-duty enough to get through a whole training job, especially if you have a large buffer size or a large number of epochs.  You can still use this notebook for training, but may need to set up an alternative VM ([learn more](https://research.google.com/colaboratory/local-runtimes.html)) for production use.  Alternatively, you can package your code for running large training jobs on Google's AI Platform [as described here](https://cloud.google.com/ml-engine/docs/tensorflow/trainer-considerations).  The following code loads a pre-trained model, which you can use for predictions right away."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#!mkdir -p drive/MyDrive/unet\n",
        "m.save('drive/MyDrive/unet/unet_10-30')"
      ],
      "metadata": {
        "id": "J5NurefaqFgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RJpNfEUS1qp"
      },
      "source": [
        "# Load a trained model. 50 epochs. 25 hours. Final RMSE ~0.08.\n",
        "#MODEL_DIR = 'gs://ee-docs-demos/fcnn-demo/trainer/model'\n",
        "#m = tf.keras.models.load_model(MODEL_DIR)\n",
        "#m.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ySNup0xCqN"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "The prediction pipeline is:\n",
        "\n",
        "1.  Export imagery on which to do predictions from Earth Engine in TFRecord format to a Cloud Storage bucket.\n",
        "2.  Use the trained model to make the predictions.\n",
        "3.  Write the predictions to a TFRecord file in a Cloud Storage.\n",
        "4.  Upload the predictions TFRecord file to Earth Engine.\n",
        "\n",
        "The following functions handle this process.  It's useful to separate the export from the predictions so that you can experiment with different models without running the export every time."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image.clip(alibori)"
      ],
      "metadata": {
        "id": "NtXN52VETwDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3WDAa-RUpXP"
      },
      "source": [
        "def doExport(out_image_base, kernel_buffer, region):\n",
        "  \"\"\"Run the image export task.  Block until complete.\n",
        "  \"\"\"\n",
        "  task = ee.batch.Export.image.toCloudStorage(\n",
        "    image = image.select(BANDS),\n",
        "    description = out_image_base,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + '/' + out_image_base,\n",
        "    region = region.getInfo()['coordinates'],\n",
        "    scale = 30,\n",
        "    fileFormat = 'TFRecord',\n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = {\n",
        "      'patchDimensions': KERNEL_SHAPE,\n",
        "      'kernelSize': kernel_buffer,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "  task.start()\n",
        "\n",
        "  # Block until the task completes.\n",
        "  print('Running image export to Cloud Storage...')\n",
        "  import time\n",
        "  while task.active():\n",
        "    time.sleep(30)\n",
        "\n",
        "  # Error condition\n",
        "  if task.status()['state'] != 'COMPLETED':\n",
        "    print('Error with image export.')\n",
        "  else:\n",
        "    print('Image export completed.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb_9_FflygVw"
      },
      "source": [
        "def doPrediction(out_image_base, user_folder, kernel_buffer, region):\n",
        "  \"\"\"Perform inference on exported imagery, upload to Earth Engine.\n",
        "  \"\"\"\n",
        "\n",
        "  print('Looking for TFRecord files...')\n",
        "\n",
        "  # Get a list of all the files in the output bucket.\n",
        "  filesList = !gsutil ls 'gs://'{BUCKET}'/'{FOLDER}\n",
        "\n",
        "  # Get only the files generated by the image export.\n",
        "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "\n",
        "  # Get the list of image files and the JSON mixer file.\n",
        "  imageFilesList = []\n",
        "  jsonFile = None\n",
        "  for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "      imageFilesList.append(f)\n",
        "    elif f.endswith('.json'):\n",
        "      jsonFile = f\n",
        "\n",
        "  # Make sure the files are in the right order.\n",
        "  imageFilesList.sort()\n",
        "\n",
        "  from pprint import pprint\n",
        "  pprint(imageFilesList)\n",
        "  print(jsonFile)\n",
        "\n",
        "  import json\n",
        "  # Load the contents of the mixer file to a JSON object.\n",
        "  jsonText = !gsutil cat {jsonFile}\n",
        "  # Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "  mixer = json.loads(jsonText.nlstr)\n",
        "  pprint(mixer)\n",
        "  patches = mixer['totalPatches']\n",
        "\n",
        "  # Get set up for prediction.\n",
        "  x_buffer = int(kernel_buffer[0] / 2)\n",
        "  y_buffer = int(kernel_buffer[1] / 2)\n",
        "\n",
        "  buffered_shape = [\n",
        "      KERNEL_SHAPE[0] + kernel_buffer[0],\n",
        "      KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
        "\n",
        "  imageColumns = [\n",
        "    tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32) \n",
        "      for k in BANDS\n",
        "  ]\n",
        "\n",
        "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
        "\n",
        "  def parse_image(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "  def toTupleImage(inputs):\n",
        "    inputsList = [inputs.get(key) for key in BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "\n",
        "   # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "\n",
        "  # Perform inference.\n",
        "  print('Running predictions...')\n",
        "  predictions = m.predict(imageDataset, steps=patches, verbose=1)\n",
        "  # print(predictions[0])\n",
        "\n",
        "  print('Writing predictions...')\n",
        "  out_image_file = 'gs://' + BUCKET + '/' + FOLDER + '/' + out_image_base + '.TFRecord'\n",
        "  writer = tf.io.TFRecordWriter(out_image_file)\n",
        "  patches = 0\n",
        "  for predictionPatch in predictions:\n",
        "    print('Writing patch ' + str(patches) + '...')\n",
        "    predictionPatch = predictionPatch[\n",
        "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
        "\n",
        "    # Create an example.\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'target': tf.train.Feature(\n",
        "              float_list=tf.train.FloatList(\n",
        "                  value=predictionPatch.flatten()))\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "    patches += 1\n",
        "\n",
        "  writer.close()\n",
        "\n",
        "  # Start the upload.\n",
        "  out_image_asset = user_folder + '/' + out_image_base\n",
        "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZqlymOehnQO"
      },
      "source": [
        "Now there's all the code needed to run the prediction pipeline, all that remains is to specify the output region in which to do the prediction, the names of the output files, where to put them, and the shape of the outputs.  In terms of the shape, the model is trained on 256x256 patches, but can work (in theory) on any patch that's big enough with even dimensions ([reference](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)).  Because of tile boundary artifacts, give the model slightly larger patches for prediction, then clip out the middle 256x256 patch.  This is controlled with a kernel buffer, half the size of which will extend beyond the kernel buffer.  For example, specifying a 128x128 kernel will append 64 pixels on each side of the patch, to ensure that the pixels in the output are taken from inputs completely covered by the kernel."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alibori = ee.FeatureCollection(\"projects/sanford-project-04a9/assets/Alibori_study_area\")\n",
        "#alibori = image.clip(alibori)"
      ],
      "metadata": {
        "id": "J36LulqmSvQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPANwc7B1-TS"
      },
      "source": [
        "# Output assets folder: YOUR FOLDER\n",
        "user_folder = 'users/jacksonpullman' # INSERT YOUR FOLDER HERE.\n",
        "\n",
        "# Base file name to use for TFRecord files and assets.\n",
        "bj_image_base = '3-2-pred'\n",
        "# Half this will extend on the sides of each patch.\n",
        "bj_kernel_buffer = [32, 32]\n",
        "\n",
        "bj_region = alibori.geometry()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLNEOLkXWvSi"
      },
      "source": [
        "# Run the export.\n",
        "doExport(bj_image_base, bj_kernel_buffer, bj_region)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxACnxKFrQ_J"
      },
      "source": [
        "# Run the prediction.\n",
        "doPrediction(bj_image_base, user_folder, bj_kernel_buffer, bj_region)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj_G9OZ1xH6K"
      },
      "source": [
        "# Display the output\n",
        "\n",
        "One the data has been exported, the model has made predictions and the predictions have been written to a file, and the image imported to Earth Engine, it's possible to display the resultant Earth Engine asset.  Here, display the predictions over Benin. NEED TO WAIT ~30 MINUTES FOR UPLOAD TO REACH EARTH ENGINE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgco6HJ4R5p2"
      },
      "source": [
        "out_image = ee.Image(user_folder + '/' + bj_image_base).clip(alibori.geometry())\n",
        "mapid = out_image.getMapId({'min': 0, 'max': 1})\n",
        "map = folium.Map(location=[9.8, 2.4], zoom_start = 7)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='predicted target',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize Training AND Testing Villages"
      ],
      "metadata": {
        "id": "bEy3NNG6mSYS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}